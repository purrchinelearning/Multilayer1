{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "690916cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer number?10\n",
      "W1:1.452954809821416e-14\n",
      "b1:9.61275272868052e-14\n",
      "W2:1.1680665413923712e-13\n",
      "b2:6.947891820398404e-13\n",
      "W3:1.2940771506280374e-13\n",
      "b3:3.204007230173829e-13\n",
      "W4:2.0713062462335653e-13\n",
      "b4:4.811131008006953e-13\n",
      "W5:2.6446816236062787e-13\n",
      "b5:8.646465103253858e-11\n",
      "W6:2.569742685531784e-13\n",
      "b6:1.737957575536654e-09\n",
      "W7:3.26173805098564e-13\n",
      "b7:1.1620366345242415e-07\n",
      "W8:5.013525276581033e-13\n",
      "b8:7.326637593114364e-11\n",
      "W9:7.2251948852351894e-12\n",
      "b9:2.5647007094925552e-05\n",
      "W10:6.659137819425636e-10\n",
      "b10:1.3946742661169463e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Net:\n",
    "    def __init__(self,input_size,hidden_size,output_size,wis=0.01):\n",
    "        self.params={}\n",
    "        self.n = int(input('layer number?' ))\n",
    "        for i in range(self.n):\n",
    "            if i == 0:\n",
    "                self.params['W{0}'.format(i+1)]=wis*np.random.randn(input_size,hidden_size)\n",
    "                self.params['b{0}'.format(i+1)]=wis*np.random.randn(hidden_size)\n",
    "            elif i == self.n-1:\n",
    "                self.params['W{0}'.format(i+1)]=wis*np.random.randn(hidden_size,output_size)\n",
    "                self.params['b{0}'.format(i+1)]=wis*np.random.randn(output_size)\n",
    "            else:\n",
    "                self.params['W{0}'.format(i+1)]=wis*np.random.randn(hidden_size,hidden_size)\n",
    "                self.params['b{0}'.format(i+1)]=wis*np.random.randn(hidden_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        for i in range(self.n):\n",
    "            if i != self.n-1:\n",
    "                self.layers['Affine{0}'.format(i+1)]=Affine(self.params['W{0}'.format(i+1)],self.params['b{0}'.format(i+1)])\n",
    "                self.layers['Relu{0}'.format(i+1)]=Relu()\n",
    "            if i == self.n-1:\n",
    "                self.layers['Affine{0}'.format(i+1)]=Affine(self.params['W{0}'.format(i+1)],self.params['b{0}'.format(i+1)])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "    \n",
    "    def predict(self,x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y,t)\n",
    "    \n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y,axis=1)\n",
    "        if t.ndim!=1:\n",
    "            t = np.argmax(t,axis=1)\n",
    "        accuracy = np.sum(y==t)/float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        for i in range(self.n):\n",
    "            grads['W{0}'.format(i+1)]=numerical_gradient(loss_W,self.params['W{0}'.format(i+1)])\n",
    "            grads['b{0}'.format(i+1)]=numerical_gradient(loss_W,self.params['b{0}'.format(i+1)])\n",
    "        return grads\n",
    "    \n",
    "    def gradient(self,x,t):\n",
    "        self.loss(x,t)\n",
    "        dout=1\n",
    "        dout=self.lastLayer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        grads = {}\n",
    "        for i in range(self.n):\n",
    "            grads['W{0}'.format(i+1)] = self.layers['Affine{0}'.format(i+1)].dW\n",
    "            grads['b{0}'.format(i+1)] = self.layers['Affine{0}'.format(i+1)].db\n",
    "        return grads\n",
    "    \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = Net(input_size=784, hidden_size=50, output_size=10)\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "# for i in range(network.n):\n",
    "#     print(network.params['W{0}'.format(i+1)].shape)\n",
    "#     print(network.params['b{0}'.format(i+1)].shape)\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8fda3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
